= Equoid
:page-link: equoid
:page-liquid:
:page-weight: 0 
:page-labels: [Scala, S2I, Infinispan, Spark, Artemis]
:page-layout: application
:page-menu_template: menu_tutorial_application.html
:page-description: Equoid is an implementation of a top-k (aka heavy hitters) tracking system built upon the notion of utilizing a Count-Min Sketch. The project demonstrates the utility of microserviced data streaming pipelines coupled with a temporal and spatial efficient approach to a common use case. The application contains a web server, web UI, caching layer, Apache Artemis broker with associated data publisher and receivers. 
:page-project_links: ["https://github.com/eldritchjs/equoid-data-publisher", "https://github.com/eldritchjs/equoid-data-handler", "https://github.com/eldritchjs/equoid-openshift", "https://github.com/Jiri-Kremser/equoid-ui"]

[[introduction]]
== Introduction

Equoid is a microservice-driven application which implements a https://en.wikipedia.org/wiki/Streaming_algorithm#Frequent_elements[heavy hitter assessment] system for https://en.wikipedia.org/wiki/Streaming_algorithm[data streams]. As will be discussed, Equoid uses https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch[Count-Min Sketch] objects as the foundation for its top-k computation. Furthermore, Equoid leverages https://spark.apache.org/[Apache Spark] for scalable processing of streaming data. 

Using the instructions that follow, you will be able to deploy Equoid on an OpenShift cluster with a simulated stream of a dataset provided in the Equoid repository. In addition, guidance is provided for using Equoid with the dataset or stream of your choosing. 

[[implementation]]
== Implementation

=== Reducing transfer and storage quantity

When thinking about keeping track of the top-k items received in a stream, it is tempting to pursue a counting approach wherein every element received has its count kept. This, while guaranteed to be correct, can be prohibitive with respect to data transfers and storage footprint. Consider a system where the streamed items to be counted have a massive or countably infinite range of values. In this case, storage must be allocated for a label and counter, each item of the stream will require a lookup and increment, and a check to see if a shuffle is needed, assuming the data is to be sorted in-place. 

Equoid, on the other hand, incorporates a Count-Min Sketch for keeping track of the heavy hitters received over the stream. Without delving into the math involved, a Count-Min Sketch is a probabilistic data structure which  provides a frequency table of events, in the top-k case these events would be a given item appearing on the stream. In terms of storage and computation it amounts to a set of hash functions mapping events to frequencies, so a table of hash functions is maintained. I said previously that the counting approach is guaranteed to be correct since it takes into account every item ever seen on the stream. In the Count-Min Sketch case, it is known that a measure of overcounting is possible and expected, however, for large scale data streams, this is usually neglible. Furthermore, parameters have been exposed in Equoid for tuning the sketches to the data stream as described further below. Another benefit of using the count-min sketch approach is the disregard of anomalous or spurious values. However, this also eliminates the potential for detecting similar but different values (e.g. itemN vs itemn.) This could be addressed by considering similarity measures of incoming values with those currently in the top-k. 

=== Data Sources

In data streaming applications, an issue that arises is the availability of quality data streams. Indeed, seldom are the organizations willing to part with their live streams gratis. One can consider a synthetic stream, wherein a dataset is transmitted piecemeal over a stream, in a temporal fashion if desired, or at random. However, this is still limited to available datasets which may not possess the characteristics one wishes to address in their solution. As such, this notion of a synthetic dataset could be extended such that controls are provided for adding randomness to the record selection and/or generation process. For illustrative purposes, Equoid has three sample use cases, all derived from the same dataset. Namely, the online retail dataset provided by the UC Irvine Machine learning library https://archive.ics.uci.edu/ml/datasets/online+retail[found here]. We address the cases where a dataset is to be passed linearly through the stream, the case where one column is chosen at random using a https://en.wikipedia.org/wiki/Zipf%27s_law[Zipfian distribution] and sent through the strea, and a case where two columns are chosen at random and sent through the stream. For convenience, we provide a file for each of these which can be provided to the data publisher.

[[architecture]]
== Microservice Architecture

Equoid is made up of a number of components giving users a means for visualizing the top-k elements in a stream during a specified temporal window. This is made possible through a hosted web UI and backend REST interface for parameter updates. 

The services of Equoid are: 

- A data publisher
- An message broker
- A data handler
- A caching layer 
- A web UI
- A REST API for adding items to the set of possibilities the data publisher chooses from 

The following diagram shows the overall architecture of Equoid.

pass:[<img src="/assets/equoid/Equoid-Architecture.png" alt="Alt text" class="img-responsive arch" width="800px">]

Equoid utilizes a caching layer for storing the top-k values. The data publisher transmits records to Artemis, the data handler retrieves mesages from the Artemis broker, parses or converts them per user specification, then adds the information to the extant top-k object. It's worth noting that multiple instances of the data publisher and data handler can co-exist, which would be akin to a production system. 

== Software Components

We chose to use Artemis for the message broker, with the expectation it would be scalable in terms of number of streams as well as volume of traffic. In the same vein, we utilized Spark for our microservices requiring scalability, namely the data handler. Since we wished to take advantage of extant Spark libraries, and the CMSketch() object is only (as of this writing) available in Java and Scala, we chose to implement the handler in Scala. In the interest of uniformity the sample data publisher is also written in Scala. Furthermore, the AMQP Spark connector used by the data handler is one developed by Paolo Patierno at Red Hat, with more information https://radanalytics.io/examples/amqpstreaming[in this tutorial] and associated https://github.com/redhat-iot/amqp-spark-demo[upstream project].

pass:[<img src="/assets/equoid/Equoid-Software-Components.png" alt="Alt text" class="img-responsive arch" width="800px">]
 
[[installation]]
== Installation

As mentioned previously, Equoid consists of a number of microservices which require deployment to an OpenShift instance. In order to get you started as painlessly as possible, in this section a walkthrough of installing and configuring the Equoid services is given. These should be followed in the order presented. These steps follow the same flow as the equoid-openshift repository's https://github.com/EldritchJS/equoid-openshift/blob/master/start-full.sh[start-full.sh] script. 

[[prerequisites]]
=== Prerequisites

You will need an OpenShift instance in place which you are able to access and create new projects on. This could be local using minishift or the oc tooling, or on an admistered cluster to which you have access. Be certain to log in to the OpenShift cluster, then you can created your project by running the following command:

....
oc new-project <YOUR_PROJECT_NAME>
....

where `<YOUR_PROJECT_NAME>` is whatever you choose to name your project (e.g. equoid)

=== Image streams and templates

Depending on your OpenShift instance, some of the image streams necessary for Equoid may not be available, to be certain you have what's necessary to build Equoid, run the following sequence of commands:

....
oc create -f https://raw.githubusercontent.com/radanalyticsio/equoid-openshift/openjdk18-image-stream.json
oc create -f https://radanalytics.io/resources.yaml
oc create -f https://raw.githubusercontent.com/infinispan/infinispan-openshift-templates/master/templates/infinispan-ephemeral.json
....

these provide the OpenJDK image stream for the data-publisher service, the radanalytics.io Oshinko et al. resources for facilitating Spark cluster creation and deployment, and the Infinispan template for the caching microservice, respectively. 

=== Launch Artemis

We'll get Artemis up and running, as it is necessary for both the publisher and handler:

....
oc create -f https://raw.githubusercontent.com/radanalyticsio/equoid-openshift/artemis-rc.yaml
....


=== Launch Infinispan Cache

We'll want our caching layer in place before we begin streaming and ingesting, the following will get an Infinispan pod running. `APPLICATION_USER`, `APPLICATION_PASSWORD`, `MANAGEMENT_USER` and `MANAGEMENT_PASSWORD` should be changed to your desired values. 

....
oc new-app --template=infinispan-ephemeral \
    -l app=datagrid \
    -p APPLICATION_NAME=datagrid \
    -p NAMESPACE=`oc project -q` \
    -p APPLICATION_USER=<YOUR_APP_USERNAME> \
    -p APPLICATION_PASSWORD=<YOUR_APP_PASSWORD> \
    -p MANAGEMENT_USER=<YOUR_MGMT_USERNAME> \
    -p MANAGEMENT_PASSWORD=<YOUR_MGMT_PASSWORD>
....

=== Launch Data Publisher

We can next begin publishing our streaming data by starting an instance of the data publisher. `OP_MODE` described later should be set to the mode you wish. Also, `DATA_URL_PRIMARY` should be set to the location of your data set or data potential values file. 

....
oc new-app \
    -l app=publisher \
    -e OP_MODE={SINGLE|DUAL|LINEAR}
    -e DATA_URL_PRIMARY=<YOUR_DATASET_URL>
    --image-stream=`oc project -q`/redhat-openjdk18-openshift:1.3 \
    https://github.com/eldritchjs/equoid-data-publisher
....


=== Launch Data Handler

We are now ready to start our data handler. Note that `INFINISPAN_HOST` and `INFINISPAN_PORT` need to be set to appropriate values, in the case of the template provided, these are `datagrid-hotrod` and `11222` respectively. In addition, `WINDOW_SECONDS` is the number of second for which you wish to track the top-k items, `SLIDE_SECONDS` represents the number of seconds to slide your data window each iteration, and `BATCH_SECONDS` is the size of samples you wish to acquire from the message stream at a time. Finally, `OP_MODE` needs to be set to the same value as set for the data publisher.

....
oc new-app --template=oshinko-scala-spark-build-dc \
    -l app=handler-20-stock \
    -p SBT_ARGS=assembly \
    -p APPLICATION_NAME=equoid-data-handler-20-stock \
    -p GIT_URI=https://github.com/eldritchjs/equoid-data-handler \
    -p GIT_REF=master \
    -p APP_MAIN_CLASS=io.radanalytics.equoid.DataHandler \
    -e INFINISPAN_HOST=<YOUR_HOSTNAME> \
    -e INFINISPAN_PORT=<YOUR_PORT> \
    -e WINDOW_SECONDS=<YOUR_WINDOW_SIZE> \
    -e SLIDE_SECONDS=<YOUR_SLIDING_SIZE> \
    -e BATCH_SECONDS=<YOUR_BATCH_SIZE> \
    -e OP_MODE={SINGLE|DUAL|LINEAR}
    -p SPARK_OPTIONS='--driver-java-options=-Dvertx.cacheDirBase=/tmp'
....

=== Launch Web UI and Set Up Keycloak

Equoid's web UI is launched using a script developed by one of Equoid's contributors, Jiri Kremser. The following calls will utilize that script to set up the UI as well as a keycloak instance for users and login. Finally, an edit role is given to the project's service account so additional data handler instances can be created/modified. 

....
BASE_URL="https://raw.githubusercontent.com/Jiri-Kremser/equoid-ui/master/ocp/"
curl -sSL $BASE_URL/ocp-apply.sh | \
    BASE_URL="$BASE_URL" \
    KC_REALM_PATH="web-ui/keycloak/realm-config" \
    bash -s stable
oc policy add-role-to-user edit system:serviceaccount:$PROJECT_NAME:default
....

[[usage]]
== Usage

We provide a number of variables to modify per the needs of end users. In this section, these are described. 

=== Data Publisher

`DATA_URL_PRIMARY` - URL of either full dataset or list of all field values for the linear and single `OP_MODE` settings, respectively.

`DATA_URL_SECONDARY` - URL of all field values for the dual `OP_MODE` setting.

`OP_MODE` - Operating mode: single for a list of field values from which random elements will be generated, dual for two lists of field values from which random elements will be generated, linear for a dataset meant to be read in sequence and transmitted.

=== Data Handler

`WINDOW_SECONDS` - Size, in seconds of window for which the top-k elements should be determined. 

`SLIDE_SECONDS` - Size, in seconds, of the amount to slide the sample window by each iteration. 

`BATCH_SECONDS` - Size, in seconds, of the batch size to be acquired from the broker.

`OP_MODE` - As with the data publisher, single for a list of field values from which random elements will be generated, dual for two lists of field values from which random elements will be generated, linear for a dataset meant to be read in sequence and transmitted.

=== Settings for Examples

Single-field sales messages generated from a set of possible values for item ID as provided by UCI dataset::
* `DATA_URL_PRIMARY` https://github.com/EldritchJS/equoid-data-publisher/blob/master/data/StockCodes.txt for data publisher 
* `OP_MODE` single for both data publisher and handler

Dual-field sales messages generated from a set of possible values for item ID and country as provided by UCI dataset::
* `DATA_URL_PRIMARY` https://github.com/EldritchJS/equoid-data-publisher/blob/master/data/StockCodes.txt for data publisher 
* `DATA_URL_SECONDARY` https://github.com/EldritchJS/equoid-data-publisher/blob/master/data/Countries.txt for data publisher 
* `OP_MODE` dual for both data publisher and handler

Single-field sales messages sent in order as provided in the UCI dataset::
* `DATA_URL_PRIMARY` https://github.com/EldritchJS/equoid-data-publisher/blob/master/data/StockCodes.txt for data publisher 
* `OP_MODE` linear for both data publisher and handler

[[expansion]]
== Expansion

=== Multi-field specification and selection

One potentially useful feature would be to provide a means for specifying a prio the format of messages such that individual fields or combinations of fields could be considered for top-k analysis. 

